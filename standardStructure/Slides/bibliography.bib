@book{Wickham2017,
author = {Wickham, Hadley and Grolemund, Garrett},
edition = {1st},
isbn = {1491910399},
publisher = {O'Reilly Media, Inc.},
title = {{R for Data Science: Import, Tidy, Transform, Visualize, and Model Data}},
year = {2017}
}
@article{Wickham2014,
abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
author = {Wickham, Hadley},
doi = {10.18637/jss.v059.i10},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Data cleaning,Data tidying,R,Relational databases},
title = {{Tidy data}},
year = {2014}
}
@article{Teso2018,
abstract = {The emergence of online user-generated content has raised numerous questions about discourse gender differences as compared to face-to-face interactions. The intended gender-free equality of Internet has been challenged by numerous studies, and significant differences have been found in online communications. This paper proposes the application of text mining techniques to online gender discourse through the analysis of shared reviews in electronic word-of-mouth communities (eWOM), which is a form of user-generated content. More specifically, linguistic issues, sentiment analysis and content analysis were applied to online reviews from a gender perspective. The methodological approach includes gathering online reviews, pre-processing collected reviews and a statistical analysis of documents features to extract the differences between male and female discourses in a specific product category. Findings reveal not only the discourse differences between women and men but also their different preferences and the feasibility of predicting gender using a set of frequent key terms. These findings are interesting both for retailers so they can adapt their offer to the gender of customers, and for online recommender systems, as the proposed methodology can be used to predict the gender of users in those cases where the gender is not explicitly stated.},
author = {Teso, E and Olmedilla, M and Mart{\'{i}}nez-Torres, M R and Toral, S L},
doi = {https://doi.org/10.1016/j.techfore.2017.12.018},
issn = {0040-1625},
journal = {Technological Forecasting and Social Change},
keywords = {Content analysis,Discourse analysis,Electronic word-of-mouth,Sentiment analysis,User-generated content,eWOM},
pages = {131--142},
title = {{Application of text mining techniques to the analysis of discourse in eWOM communications from a gender perspective}},
url = {http://www.sciencedirect.com/science/article/pii/S0040162517303256},
volume = {129},
year = {2018}
}
@book{Silge2017,
abstract = {Chapter 7. Case Study: Comparing Twitter Archives; Getting the Data and Distribution of Tweets; Word Frequencies; Comparing Word Usage; Changes in Word Use; Favorites and Retweets; Summary; Chapter 8. Case Study: Mining NASA Metadata; How Data Is Organized at NASA; Wrangling and Tidying the Data; Some Initial Simple Exploration; Word Co-ocurrences and Correlations; Networks of Description and Title Words; Networks of Keywords; Calculating tf-idf for the Description Fields; What Is tf-idf for the Description Field Words?; Connecting Description Fields to Keywords; Topic Modeling},
address = {Sebastopol, CA},
author = {Silge, Julia and Robinson, David},
isbn = {9781491981627 1491981628 9781491981603 1491981601},
language = {English},
publisher = {O'Reilly Media},
title = {{Text mining with R: A tidy approach}},
year = {2017}
}
@article{Michel2011,
abstract = {We constructed a corpus of digitized texts containing about 4{\%} of all books ever printed. Analysis of this corpus enables us to investigate cultural trends quantitatively. We survey the vast terrain of 'culturomics,' focusing on linguistic and cultural phenomena that were reflected in the English language between 1800 and 2000. We show how this approach can provide insights about fields as diverse as lexicography, the evolution of grammar, collective memory, the adoption of technology, the pursuit of fame, censorship, and historical epidemiology. Culturomics extends the boundaries of rigorous quantitative inquiry to a wide array of new phenomena spanning the social sciences and the humanities.},
author = {Michel, Jean Baptiste and {Kui Shen}, Yuan and {Presser Aiden}, Aviva and Veres, Adrian and Gray, Matthew K. and Pickett, Joseph P. and Hoiberg, Dale and Clancy, Dan and Norvig, Peter and Orwant, Jon and Pinker, Steven and Nowak, Martin A. and Aiden, Erez Lieberman},
doi = {10.1126/science.1199644},
issn = {00368075},
journal = {Science},
pages = {176--182},
title = {{Quantitative analysis of culture using millions of digitized books}},
volume = {331},
year = {2011}
}
@article{Lansdall-Welfare2017,
abstract = {Previous studies have shown that it is possible to detect macroscopic patterns of cultural change over periods of centuries by analyzing large textual time series, specifically digitized books. This method promises to empower scholars with a quantitative and data-driven tool to study culture and society, but its power has been limited by the use of data from books and simple analytics based essentially on word counts. This study addresses these problems by assembling a vast corpus of regional newspapers from the United Kingdom, incorporating very fine-grained geographical and temporal information that is not available for books. The corpus spans 150 years and is formed by millions of articles, representing 14{\%} of all British regional outlets of the period. Simple content analysis of this corpus allowed us to detect specific events, like wars, epidemics, coronations, or conclaves, with high accuracy, whereas the use of more refined techniques from artificial intelligence enabled us to move beyond counting words by detecting references to named entities. These techniques allowed us to observe both a systematic underrepresentation and a steady increase of women in the news during the 20th century and the change of geographic focus for various concepts. We also estimate the dates when electricity overtook steam and trains overtook horses as a means of transportation, both around the year 1900, along with observing other cultural transitions. We believe that these data-driven approaches can complement the traditional method of close reading in detecting trends of continuity and change in historical corpora.},
author = {Lansdall-Welfare, Thomas and Sudhahar, Saatviga and Thompson, James and Lewis, Justin and {FindMyPast Newspaper Team}, FindMyPast Newspaper and Cristianini, Nello},
doi = {10.1073/pnas.1606380114},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Culturomics,artificial intelligence,computational history,data science,digital humanities},
month = {jan},
number = {4},
pages = {E457--E465},
pmid = {28069962},
publisher = {National Academy of Sciences},
title = {{Content analysis of 150 years of British periodicals.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28069962 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5278459},
volume = {114},
year = {2017}
}
@book{Kwartler2017,
abstract = {A reliable, cost-effective approach to extracting priceless business information from all sources of text Excavating actionable business insights from data is a complex undertaking, and that complexity is magnified by an order of magnitude when the focus is on documents and other text information. This book takes a practical, hands-on approach to teaching you a reliable, cost-effective approach to mining the vast, untold riches buried within all forms of text using R. Author Ted Kwartler clearly describes all of the tools needed to perform text mining and shows you how to use them to identify practical business applications to get your creative text mining efforts started right away. With the help of numerous real-world examples and case studies from industries ranging from healthcare to entertainment to telecommunications, he demonstrates how to execute an array of text mining processes and functions, including sentiment scoring, topic modelling, predictive modelling, extracting clickbait from headlines, and more. You ll learn how to: Identify actionable social media posts to improve customer service Use text mining in HR to identify candidate perceptions of an organisation, match job descriptions with resumes, and more Extract priceless information from virtually all digital and print sources, including the news media, social media sites, PDFs, and even JPEG and GIF image files Make text mining an integral component of marketing in order to identify brand evangelists, impact customer propensity modelling, and much more Most companies data mining efforts focus almost exclusively on numerical and categorical data, while text remains a largely untapped resource. Especially in a global marketplace where being first to identify and respond to customer needs and expectations imparts an unbeatable competitive advantage, text represents a source of immense potential value. Unfortunately, there is no reliable, cost-effective technology for extracting analytical insights from the huge and ever-growing volume of text available online and other digital sources, as well as from paper documents until now.},
address = {Chichester, United Kingdom},
author = {Kwartler, Ted},
booktitle = {Text Mining in Practice with R},
doi = {10.1002/9781119282105},
publisher = {John Wiley {\&} Sons Ltd},
title = {{Text Mining in Practice with R}},
year = {2017}
}
@article{Kim2015a,
abstract = {Because of the accelerated life cycle in technology and correspondingly rapid technological saturation in markets, firms are not only accelerating the rate of technological innovation but also expanding the scope of their products or services by combining product or service features of other markets, which eventually leads to industry convergence. However, despite the significant impact of industry convergence on the economy, our understanding of the phenomenon is still limited because previous studies explored only a few cases and come largely from the technological perspective. Therefore, it is still questionable whether industry convergence is a general phenomenon that is prevalent across entire industries. In this paper, we analyze the phenomenon in entire U.S. industries, focusing on its trends and patterns. To do so, we conduct a co-occurrence-based analysis of text mining for a large volume of unstructured data – 2 million newspaper articles from 1989 to 2012 – and suggest using an industry convergence (IC) index based on normalized pointwise mutual information (PMI). We find that overall industry convergence is increasing over time. Moreover, the rate of the increase has been greater within industry than between industries at a given industry level. However, when we cluster the dynamic patterns of industry convergence among industry pairs, the patterns are mixed, and, while some industry groups are converging over time, others are stationary. These findings suggest that significant transformation is under way in the economy, but this phenomenon is not yet prevalent across entire industries. In addition, this study provides a method for anticipating the future direction of industry convergence.},
author = {Kim, Namil and Lee, Hyeokseong and Kim, Wonjoon and Lee, Hyunjong and Suh, Jong Hwan},
doi = {https://doi.org/10.1016/j.respol.2015.02.001},
issn = {0048-7333},
journal = {Research Policy},
keywords = {Co-occurrence-based analysis,Industry convergence,Industry convergence index,Industry convergence map,Unstructured data},
number = {9},
pages = {1734--1748},
title = {{Dynamic patterns of industry convergence: Evidence from a large amount of unstructured data}},
url = {http://www.sciencedirect.com/science/article/pii/S0048733315000220},
volume = {44},
year = {2015}
}
@article{Iliev2016,
abstract = {For nearly 50 y social scientists have observed that across cultures and languages people use more positive words than negative words, a phenomenon referred to as “linguistic positivity bias” (LPB). Although scientists have proposed multiple explanations for this phenomenon—explanations that hinge on mechanisms ranging from cognitive biases to environmental factors—no consensus on the origins of LPB has been reached. In this research, we derive and test, via natural language processing and data aggregation, divergent predictions from dominant explanations of LPB by examining it across time. We find that LPB varies across time and therefore cannot be explained simply as the product of cognitive biases and, further, that these variations correspond to fluctuations in objective circumstances and subjective mood.},
author = {Iliev, Rumen and Hoover, Joe and Dehghani, Morteza and Axelrod, Robert},
doi = {10.1073/PNAS.1612058113},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
month = {dec},
number = {49},
pages = {E7871--E7879},
pmid = {27872286},
publisher = {National Academy of Sciences},
title = {{Linguistic positivity in historical texts reflects dynamic environmental and psychological factors}},
url = {http://www.pnas.org/content/113/49/E7871},
volume = {113},
year = {2016}
}
@article{Hofmann2019,
abstract = {Ongoing advances in digital technologies – which enable new products, services, and business models – have fundamentally affected business and society through several waves of digitalization. When analyzing digital technologies, a dynamic system or an ecosystem model that represents interrelated technologies is beneficial owing to the systemic character of digital technologies. Using an assembly-based process model for situational method engineering, and following the design science research paradigm, we develop an analytical method to generate technology-related network data that retraces elapsed patterns of technological change. We consider the technological distances that characterize technologies' proximities and dependencies. We use established text mining techniques and draw from technology innovation research as justificatory knowledge. The proposed method processes textual data from different information sources into an analyzable and readable inter-technology relationship network. To evaluate the method, we use exemplary digital technologies from the big data analytics domain as an application scenario.},
author = {Hofmann, Peter and Keller, Robert and Urbach, Nils},
doi = {10.1016/j.techfore.2019.02.009},
issn = {00401625},
journal = {Technological Forecasting and Social Change},
keywords = {Method construction,Network,Patent mining,Tech mining,Text mining},
number = {June},
pages = {202--213},
title = {{Inter-technology relationship networks: Arranging technologies through text mining}},
volume = {143},
year = {2019}
}
@book{Feldman2006,
abstract = {We study a family of "classical" orthogonal polynomials which satisfy (apart from a 3-term recurrence relation) an eigenvalue problem with a differential operator of Dunkl-type. These polynomials can be obtained from the little {\$}q{\$}-Jacobi polynomials in the limit {\$}q=-1{\$}. We also show that these polynomials provide a nontrivial realization of the Askey-Wilson algebra for {\$}q=-1{\$}.},
address = {Cambridge, United Kingdom},
author = {Feldman, Ronen and Sanger, James},
booktitle = {The Text Mining Handbook},
doi = {10.1017/cbo9780511546914},
publisher = {Cambridge University Press},
title = {{The Text Mining Handbook}},
year = {2006}
}
@article{Ciarli2019,
abstract = {To what extent is scientific research related to societal needs? To answer this crucial question systematically we need to contrast indicators of research priorities with indicators of societal needs. We focus on rice research and technology between 1983 and 2012. We combine quantitative methods that allow investigation of the relation between ‘revealed' research priorities and ‘revealed' societal demands, measured respectively by research output (publications) and national accounts of rice use and farmers' and consumers' rice-related needs. We employ new bibliometric data, methods and indicators to identify countries' main rice research topics (priorities) from publications. For a panel of countries, we estimate the relation between revealed research priorities and revealed demands. We find that, across countries and time, societal demands explain a country's research trajectory to a limited extent. Some research priorities are nicely aligned to societal demands, confirming that science is partly related to societal needs. However, we find a relevant number of misalignments between the focus of rice research and revealed demands, crucially related to human consumption and nutrition. We discuss some implications for research policy.},
author = {Ciarli, Tommaso and R{\`{a}}fols, Ismael},
doi = {https://doi.org/10.1016/j.respol.2018.10.027},
issn = {0048-7333},
journal = {Research Policy},
keywords = {Agenda setting,Research priority,Research trajectories,Rice,Scientometrics,Societal needs},
number = {4},
pages = {949--967},
title = {{The relation between research priorities and societal demands: The case of rice}},
url = {http://www.sciencedirect.com/science/article/pii/S0048733318302671},
volume = {48},
year = {2019}
}
@article{Callon1991,
abstract = {Abstract  The goal of this paper is to show how co-word analysis techniques can be used to study interactions between academic and technological research. It is based upon a systematic content analysis of publications in the polymer science field over a period of 15 years. The results concern a.) the evolution of research in different subject areas and the patterns of their interaction; b.) a description of subject area life cycles; c.) an analysis of research trajectories given factors of stability and change in a research network; d.) the need to use both science push and technology pull theories to explain the interaction dynamics of a research field. The co-word techniques developed in this paper should help to build a bridge between research in scientometrics and work underway to better understand the economics of innovation.},
author = {Callon, M. and Courtial, J. P. and Laville, F.},
doi = {10.1007/BF02019280},
isbn = {0138913015882861},
issn = {01389130},
journal = {Scientometrics},
month = {sep},
number = {1},
pages = {155--205},
title = {{Co-word analysis as a tool for describing the network of interactions between basic and technological research: The case of polymer chemsitry}},
url = {http://link.springer.com/10.1007/BF02019280},
volume = {22},
year = {1991}
}
@unpublished{Bone2020,
author = {Bone, Frederique and Rotolo, Daniele},
institution = {Working Paper},
title = {{Text mining historical sources to trace technological change. The case of mass production}},
year = {2020}
}
@article{Blei2012,
author = {Blei, David M.},
doi = {10.1145/2133806.2133826},
issn = {00010782},
journal = {Communications of the ACM},
number = {4},
pages = {77--84},
title = {{Probabilistic topic models}},
volume = {55},
year = {2012}
}
@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Edu, Blei@cs Berkeley and Ng, Andrew Y and Edu, Ang@cs Stanford and Jordan, Michael I and Edu, Jordan@cs Berkeley},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
volume = {3},
year = {2003}
}
